# Copyright (c) 2025 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Optional

import paddle
import triton.language as tl
from paddle import _C_ops
from paddle.base.framework import OpProtoHolder
from paddle.base.layer_helper import LayerHelper
from paddle.framework import in_dynamic_or_pir_mode

from .triton_utils import get_dtype_str, paddle_use_triton, rendering_common_template


# sage attention #
@paddle_use_triton(key=["1"])
def sageattn_quant_per_block_int8_kernel(
    Input,
    Output,
    Scale,
    L,
    stride_iz,
    stride_ih,
    stride_in,
    stride_oz,
    stride_oh,
    stride_on,
    stride_sz,
    stride_sh,
    sm_scale,
    bsz,  # grid num, through compiling
    h_attn: tl.constexpr,  # grid num, through compiling
    C: tl.constexpr,
    BLK: tl.constexpr,
):
    off_blk = tl.program_id(axis=0)
    off_h = tl.program_id(axis=1)
    off_b = tl.program_id(axis=2)

    offs_n = off_blk * BLK + tl.arange(0, BLK)
    offs_k = tl.arange(0, C)

    input_ptrs = Input + off_b * stride_iz + off_h * stride_ih + offs_n[:, None] * stride_in + offs_k[None, :]
    output_ptrs = Output + off_b * stride_oz + off_h * stride_oh + offs_n[:, None] * stride_on + offs_k[None, :]
    scale_ptrs = Scale + off_b * stride_sz + off_h * stride_sh + off_blk

    x_data = tl.load(input_ptrs, mask=offs_n[:, None] < L)
    x_data = x_data.to(tl.float32)
    x_data *= sm_scale
    scale = tl.max(tl.abs(x_data)) / 127.0
    x_int8 = x_data / scale
    x_int8 += 0.5 * tl.where(x_int8 >= 0, 1, -1)
    x_int8 = x_int8.to(tl.int8)
    tl.store(output_ptrs, x_int8, mask=offs_n[:, None] < L)
    tl.store(scale_ptrs, scale)


# per-block quant triton API
def sageattn_quant_per_block_int8(x, km=None, BLK=128, sm_scale=1.0, tensor_layout="HND"):
    """
    [params]
        x: paddle.Tensor, dtype in fp16 or bf16, this is usually q or k input tensor.
        km: paddle.Tensor, the mean tensor of k tensor. Must be provided when the `x` is k tensor.
        BLK: int, the BLK for computing q & k tensor. Default 128 for q, 64 for k, which is an optimized value.
        sm_scale: float, the scale factor for dynamic quant.
        tensor_layout: string. Only in ['HND', 'NHD'], 'HND' -> [bsz, num_heads, seq_len, head_dim],
                                                       'NHD' -> [bsz, seq_len, num_heads, head_dim]
    [Examples]
        batch_size = 2
        num_heads = 24
        seq_len = 1376
        head_dim = 64

        sm_scale = 1.0 / (head_dim_og ** 0.5)

        # note: this layout is 'NHD'
        tensor_layout = 'NHD'
        q = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")
        k = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")
        v = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")

        km = paddle.mean(k, axis=seq_dim, keepdim=True)

        q_int8, q_scale = sageattn_quant_per_block_int8(
            q, km=None, BLK=BLKQ, sm_scale=sm_scale, tensor_layout=tensor_layout)
        k_int8, k_scale = sageattn_quant_per_block_int8(
            k, km=km, BLK=BLKK, sm_scale=sm_scale, tensor_layout=tensor_layout)
    """
    if km is not None:
        x = x - km

    if tensor_layout == "HND":
        b, h_attn, seq_len, head_dim = x.shape

        # there is no `stride` attribute in static mode, so we need to compute it manually
        stride_iz, stride_ih, stride_in = head_dim * seq_len * h_attn, head_dim * seq_len, head_dim * 1
        stride_oz, stride_oh, stride_on = head_dim * seq_len * h_attn, head_dim * seq_len, head_dim * 1
    elif tensor_layout == "NHD":
        b, seq_len, h_attn, head_dim = x.shape

        stride_iz, stride_ih, stride_in = head_dim * seq_len * h_attn, head_dim * 1, head_dim * h_attn
        stride_oz, stride_oh, stride_on = head_dim * seq_len * h_attn, head_dim * 1, head_dim * h_attn
    else:
        raise ValueError(f"Unknown tensor layout: {tensor_layout}")

    if sm_scale is None:
        sm_scale = head_dim**-0.5

    L = seq_len
    C = head_dim
    sm_scale = sm_scale * 1.44269504 if km is None else 1.0

    stride_sz = h_attn * ((seq_len + BLK - 1) // BLK)
    stride_sh = (seq_len + BLK - 1) // BLK

    prepare_attr_for_triton_kernel = """
    auto output_tensor = paddle::empty(x.shape(), paddle::DataType::INT8, x.place());

    auto input_tensor = x;
    auto input_shape = x.shape();

    // define params
    int b, h_attn, seq_len, head_dim;
    int stride_iz, stride_ih, stride_in;
    int stride_oz, stride_oh, stride_on;

    // allocate
    if (tensor_layout == std::string("HND")) {
        // tensor layout unpack
        b = input_shape[0];
        h_attn = input_shape[1];
        seq_len = input_shape[2];
        head_dim = input_shape[3];

        // stride unpack
        auto tensor_strides = input_tensor.strides();
        stride_iz = tensor_strides[0];
        stride_ih = tensor_strides[1];
        stride_in = tensor_strides[2];

        auto tensor_o_strides = output_tensor.strides();
        stride_oz = tensor_o_strides[0];
        stride_oh = tensor_o_strides[1];
        stride_on = tensor_o_strides[2];

    } else if (tensor_layout == std::string("NHD")) {
        // tensor layout unpack
        b = input_shape[0];
        h_attn = input_shape[2];    // reverse
        seq_len = input_shape[1];
        head_dim = input_shape[3];

        // stride unpack
        auto tensor_strides = input_tensor.strides();
        stride_iz = tensor_strides[0];
        stride_ih = tensor_strides[2];    // reverse
        stride_in = tensor_strides[1];

        auto tensor_o_strides = output_tensor.strides();
        stride_oz = tensor_o_strides[0];
        stride_oh = tensor_o_strides[2];  // reverse
        stride_on = tensor_o_strides[1];
    }
    else {
        throw std::runtime_error("Unsupported tensor layout");
    }

    auto scale_tensor = paddle::empty({b, h_attn, (seq_len + BLK - 1) / BLK},
                                        paddle::DataType::FLOAT32, x.place());
    int L = seq_len;
    int stride_sz = scale_tensor.strides()[0];
    int stride_sh = scale_tensor.strides()[1];
    int bsz = b;
"""

    op_name = "triton_sageattn_quant_per_block"
    op_name += get_dtype_str(x.dtype)
    op_name += f"_BLK{BLK}_seq{seq_len}_h{h_attn}_dim{head_dim}"

    if op_name not in OpProtoHolder.instance().op_proto_map.keys():
        Output = paddle.empty(x.shape, dtype=paddle.int8)
        Scale = paddle.empty((b, h_attn, (seq_len + BLK - 1) // BLK), dtype="float32")
        # output_tensor & scale_tensor has beed defined in above areas
        prepare_ptr_for_triton_kernel = """
    // prepare tensor
    CUdeviceptr input_ptrs[3] = {
        get_tensor_ptr(x),
        get_tensor_ptr(output_tensor),
        get_tensor_ptr(scale_tensor)
    };
"""
        return_tensor_names = "output_tensor, scale_tensor"

        template_used = rendering_common_template(
            sageattn_quant_per_block_int8,
            prepare_attr_for_triton_kernel=prepare_attr_for_triton_kernel,
            prepare_ptr_for_triton_kernel=prepare_ptr_for_triton_kernel,
            return_tensor_names=return_tensor_names,
        )
        grid = ("(L + BLK - 1) / BLK", "h_attn", "bsz")
        sageattn_quant_per_block_int8_kernel[(op_name, template_used, grid)](
            Input=x,
            Output=Output,
            Scale=Scale,
            L=L,
            stride_iz=stride_iz,
            stride_ih=stride_ih,
            stride_in=stride_in,
            stride_oz=stride_oz,
            stride_oh=stride_oh,
            stride_on=stride_on,
            stride_sz=stride_sz,
            stride_sh=stride_sh,
            sm_scale=sm_scale,
            bsz=-1,  # grid num, for compiling
            h_attn=h_attn,  # grid num, for compiling
            C=C,
            BLK=BLK,
        )

    if in_dynamic_or_pir_mode():
        outs = _C_ops._run_custom_op(op_name, x, km, BLK, sm_scale, tensor_layout)
        return outs[0], outs[1]
    else:
        helper = LayerHelper(op_name, **locals())
        inputs = {
            "x": x,
            "km@OPTIONAL": km,
        }
        out_int8 = helper.create_variable_for_type_inference(dtype=paddle.int8)
        out_scale = helper.create_variable_for_type_inference(dtype=paddle.float32)

        helper.append_op(
            type=op_name,
            inputs=inputs,
            attrs={
                "BLK": BLK,
                "sm_scale": sm_scale,
                "tensor_layout": tensor_layout,
            },
            outputs={"output_tensor": out_int8, "scale_tensor": out_scale},
        )
        return out_int8, out_scale


def per_block_int8(q, k, km=None, BLKQ=128, BLKK=64, sm_scale=None, tensor_layout="HND"):
    q_int8, q_scale = sageattn_quant_per_block_int8(
        q, km=None, BLK=BLKQ, sm_scale=sm_scale, tensor_layout=tensor_layout
    )
    k_int8, k_scale = sageattn_quant_per_block_int8(k, km=km, BLK=BLKK, sm_scale=sm_scale, tensor_layout=tensor_layout)
    return q_int8, q_scale, k_int8, k_scale


@paddle_use_triton(key=["1"])
def sageattn_quant_query_per_thread_int8_kernel(
    Input,
    Output,
    Scale,
    L,
    stride_iz,
    stride_ih,
    stride_in,
    stride_oz,
    stride_oh,
    stride_on,
    stride_sz,
    stride_sh,
    bsz,
    h_qo: tl.constexpr,
    C: tl.constexpr,
    BLK: tl.constexpr,
    WARP: tl.constexpr,
    BKG: tl.constexpr,
):
    off_blk = tl.program_id(0) // 8
    off_tld = tl.program_id(0) % 8
    off_h = tl.program_id(1)
    off_b = tl.program_id(2)

    offs_n = off_blk * BLK + tl.arange(0, BLK // 8) * 8 + off_tld
    offs_k = tl.arange(0, C)

    input_ptrs = Input + off_b * stride_iz + off_h * stride_ih + offs_n[:, None] * stride_in + offs_k[None, :]
    output_ptrs = Output + off_b * stride_oz + off_h * stride_oh + offs_n[:, None] * stride_on + offs_k[None, :]
    scale_ptrs = Scale + off_b * stride_sz + off_h * stride_sh + off_blk * 8 + off_tld

    x = tl.load(input_ptrs, mask=offs_n[:, None] < L)
    x = x.to(tl.float32)
    scale = tl.max(tl.abs(x)) / 127.0 + 0.0000001
    x_int8 = x / scale
    x_int8 += 0.5 * tl.where(x_int8 >= 0, 1, -1)
    x_int8 = x_int8.to(tl.int8)
    tl.store(output_ptrs, x_int8, mask=offs_n[:, None] < L)
    tl.store(scale_ptrs, scale)


def sageattn_quant_query_per_thread_int8(x, BLK=128, WARP=32, sm_scale=1.0, tensor_layout="HND"):
    """
    [params]
        x: paddle.Tensor, dtype in fp16 or bf16, this is usually q or k input tensor.
        BLK: int, the BLK for computing q tensor. Default 128 for q, 64 for k, which is an optimized value.
        WARP: int, the WARP for computing q. Default 32.
        sm_scale: float, the scale factor for dynamic quant.
        tensor_layout: string. Only in ['HND', 'NHD'], 'HND' -> [bsz, num_heads, seq_len, head_dim],
                                                       'NHD' -> [bsz, seq_len, num_heads, head_dim]
    """
    if tensor_layout == "HND":
        b, h_qo, seq_len, head_dim = x.shape

        # there is no `stride` attribute in static mode, so we need to compute it manually
        stride_bz_q, stride_h_q, stride_seq_q = head_dim * seq_len * h_qo, head_dim * seq_len, head_dim * 1
        stride_bz_qo, stride_h_qo, stride_seq_qo = head_dim * seq_len * h_qo, head_dim * seq_len, head_dim * 1
    elif tensor_layout == "NHD":
        b, seq_len, h_qo, head_dim = x.shape

        stride_bz_q, stride_h_q, stride_seq_q = head_dim * seq_len * h_qo, head_dim * 1, head_dim * h_qo
        stride_bz_qo, stride_h_qo, stride_seq_qo = head_dim * seq_len * h_qo, head_dim * 1, head_dim * h_qo
    else:
        raise ValueError(f"Unknown tensor layout: {tensor_layout}")

    if sm_scale is None:
        sm_scale = head_dim**-0.5

    L = seq_len
    C = head_dim

    stride_sz = h_qo * ((seq_len + BLK - 1) // BLK) * (BLK // WARP) * 8
    stride_sh = ((seq_len + BLK - 1) // BLK) * (BLK // WARP) * 8

    prepare_attr_for_triton_kernel = """
    auto output_tensor = paddle::empty(x.shape(), paddle::DataType::INT8, x.place());

    auto input_tensor = x;
    auto input_shape = x.shape();

    // define params
    int b, h_qo, seq_len, head_dim;
    int stride_iz, stride_ih, stride_in;
    int stride_oz, stride_oh, stride_on;

    // allocate
    if (tensor_layout == std::string("HND")) {
        // tensor layout unpack
        b = input_shape[0];
        h_qo = input_shape[1];
        seq_len = input_shape[2];
        head_dim = input_shape[3];

        // stride unpack
        auto tensor_strides = input_tensor.strides();
        stride_iz = tensor_strides[0];
        stride_ih = tensor_strides[1];
        stride_in = tensor_strides[2];

        auto tensor_o_strides = output_tensor.strides();
        stride_oz = tensor_o_strides[0];
        stride_oh = tensor_o_strides[1];
        stride_on = tensor_o_strides[2];

    } else if (tensor_layout == std::string("NHD")) {
        // tensor layout unpack
        b = input_shape[0];
        h_qo = input_shape[2];    // reverse
        seq_len = input_shape[1];
        head_dim = input_shape[3];

        // stride unpack
        auto tensor_strides = input_tensor.strides();
        stride_iz = tensor_strides[0];
        stride_ih = tensor_strides[2];    // reverse
        stride_in = tensor_strides[1];

        auto tensor_o_strides = output_tensor.strides();
        stride_oz = tensor_o_strides[0];
        stride_oh = tensor_o_strides[2];  // reverse
        stride_on = tensor_o_strides[1];
    }
    else {
        throw std::runtime_error("Unsupported tensor layout");
    }

    auto scale_tensor = paddle::empty({b, h_qo, (seq_len + BLK - 1) / BLK * (BLK / WARP) * 8},
                                        paddle::DataType::FLOAT32, x.place());
    int L = seq_len;
    int stride_sz = scale_tensor.strides()[0];
    int stride_sh = scale_tensor.strides()[1];
    int bsz = b;
"""

    op_name = "triton_sageattn_quant_query_per_thread"
    op_name += get_dtype_str(x.dtype)
    op_name += f"_BLK{BLK}_seq{seq_len}_h{h_qo}_dim{head_dim}"

    if op_name not in OpProtoHolder.instance().op_proto_map.keys():
        Output = paddle.empty(x.shape, dtype=paddle.int8)
        Scale = paddle.empty(shape=(b, h_qo, ((seq_len + BLK - 1) // BLK) * (BLK // WARP) * 8), dtype="float32")
        # output_tensor & scale_tensor has beed defined in above areas
        prepare_ptr_for_triton_kernel = """
    // prepare tensor
    CUdeviceptr input_ptrs[3] = {
        get_tensor_ptr(x),
        get_tensor_ptr(output_tensor),
        get_tensor_ptr(scale_tensor)
    };
"""
        return_tensor_names = "output_tensor, scale_tensor"

        template_used = rendering_common_template(
            sageattn_quant_query_per_thread_int8,
            prepare_attr_for_triton_kernel=prepare_attr_for_triton_kernel,
            prepare_ptr_for_triton_kernel=prepare_ptr_for_triton_kernel,
            return_tensor_names=return_tensor_names,
        )
        grid = ("((L + BKG - 1) / BKG) * (BKG / WARP) * 8", "h_qo", "bsz")
        sageattn_quant_query_per_thread_int8_kernel[(op_name, template_used, grid)](
            Input=x,
            Output=Output,
            Scale=Scale,
            L=L,
            stride_iz=stride_bz_q,
            stride_ih=stride_h_q,
            stride_in=stride_seq_q,
            stride_oz=stride_bz_qo,
            stride_oh=stride_h_qo,
            stride_on=stride_seq_qo,
            stride_sz=stride_sz,
            stride_sh=stride_sh,
            bsz=-1,  # grid num, for compiling
            h_qo=h_qo,  # grid num, for compiling
            C=C,
            BLK=WARP,
            WARP=WARP,  # grid num, for compiling
            BKG=BLK,  # grid num, for compiling
        )

    if in_dynamic_or_pir_mode():
        outs = _C_ops._run_custom_op(op_name, x, BLK, WARP, sm_scale, tensor_layout)
        return outs[0], outs[1]
    else:
        helper = LayerHelper(op_name, **locals())
        inputs = {"x": x}
        out_int8 = helper.create_variable_for_type_inference(dtype=paddle.int8)
        out_scale = helper.create_variable_for_type_inference(dtype=paddle.float32)

        helper.append_op(
            type=op_name,
            inputs=inputs,
            attrs={
                "BLK": BLK,
                "WARP": WARP,
                "sm_scale": sm_scale,
                "tensor_layout": tensor_layout,
            },
            outputs={"output_tensor": out_int8, "scale_tensor": out_scale},
        )
        return out_int8, out_scale


@paddle_use_triton(key=["1"])
def sageattn_quant_key_per_thread_int8_kernel(
    Input,
    Output,
    Scale,
    L,
    stride_iz,
    stride_ih,
    stride_in,
    stride_oz,
    stride_oh,
    stride_on,
    stride_sz,
    stride_sh,
    bsz,
    h_kv: tl.constexpr,
    C: tl.constexpr,
    BLK: tl.constexpr,
    WARP: tl.constexpr,
    BKG: tl.constexpr,
):
    off_blk = tl.program_id(0) // 4
    off_tld = tl.program_id(0) % 4
    off_h = tl.program_id(1)
    off_b = tl.program_id(2)

    offs_n0 = off_blk * BLK + tl.arange(0, BLK // 8) * 8 + off_tld * 2
    offs_n1 = off_blk * BLK + tl.arange(0, BLK // 8) * 8 + off_tld * 2 + 1
    offs_k = tl.arange(0, C)

    input_ptrs0 = Input + off_b * stride_iz + off_h * stride_ih + offs_n0[:, None] * stride_in + offs_k[None, :]
    input_ptrs1 = Input + off_b * stride_iz + off_h * stride_ih + offs_n1[:, None] * stride_in + offs_k[None, :]
    output_ptrs0 = Output + off_b * stride_oz + off_h * stride_oh + offs_n0[:, None] * stride_on + offs_k[None, :]
    output_ptrs1 = Output + off_b * stride_oz + off_h * stride_oh + offs_n1[:, None] * stride_on + offs_k[None, :]
    scale_ptrs = Scale + off_b * stride_sz + off_h * stride_sh + off_blk * 4 + off_tld

    x0 = tl.load(input_ptrs0, mask=offs_n0[:, None] < L)
    x1 = tl.load(input_ptrs1, mask=offs_n1[:, None] < L)
    x0 = x0.to(tl.float32)
    x1 = x1.to(tl.float32)
    scale = max(tl.max(tl.abs(x0)), tl.max(tl.abs(x1))) / 127.0 + 0.0000001
    x0_int8 = x0 / scale
    x1_int8 = x1 / scale
    x0_int8 += 0.5 * tl.where(x0_int8 >= 0, 1, -1)
    x1_int8 += 0.5 * tl.where(x1_int8 >= 0, 1, -1)
    x0_int8 = x0_int8.to(tl.int8)
    x1_int8 = x1_int8.to(tl.int8)
    tl.store(output_ptrs0, x0_int8, mask=offs_n0[:, None] < L)
    tl.store(output_ptrs1, x1_int8, mask=offs_n1[:, None] < L)
    tl.store(scale_ptrs, scale)


def sageattn_quant_key_per_thread_int8(x, km, BLK=64, WARP=64, sm_scale=1.0, tensor_layout="HND"):
    """
    [params]
        x: paddle.Tensor, dtype in fp16 or bf16, this is usually q or k input tensor.
        km: paddle.Tensor, the mean in seq_dim of tensor K.
        BLK: int, the BLK for computing q tensor. Default 128 for q, 64 for k, which is an optimized value.
        WARP: int, the WARP for computing q. Default 64.
        sm_scale: float, the scale factor for dynamic quant.
        tensor_layout: string. Only in ['HND', 'NHD'], 'HND' -> [bsz, num_heads, seq_len, head_dim],
                                                       'NHD' -> [bsz, seq_len, num_heads, head_dim]
    """
    if tensor_layout == "HND":
        b, h_kv, seq_len, head_dim = x.shape

        # there is no `stride` attribute in static mode, so we need to compute it manually
        stride_bz_k, stride_h_k, stride_seq_k = head_dim * seq_len * h_kv, head_dim * seq_len, head_dim * 1
        stride_bz_ko, stride_h_ko, stride_seq_ko = head_dim * seq_len * h_kv, head_dim * seq_len, head_dim * 1
    elif tensor_layout == "NHD":
        b, seq_len, h_kv, head_dim = x.shape

        stride_bz_k, stride_h_k, stride_seq_k = head_dim * seq_len * h_kv, head_dim * 1, head_dim * h_kv
        stride_bz_ko, stride_h_ko, stride_seq_ko = head_dim * seq_len * h_kv, head_dim * 1, head_dim * h_kv
    else:
        raise ValueError(f"Unknown tensor layout: {tensor_layout}")

    if sm_scale is None:
        sm_scale = head_dim**-0.5

    x = x - km

    L = seq_len
    C = head_dim

    stride_sz = h_kv * ((seq_len + BLK - 1) // BLK) * (BLK // WARP) * 4
    stride_sh = ((seq_len + BLK - 1) // BLK) * (BLK // WARP) * 4

    prepare_attr_for_triton_kernel = """
    auto output_tensor = paddle::empty(x.shape(), paddle::DataType::INT8, x.place());

    auto input_tensor = x;
    auto input_shape = x.shape();

    // define params
    int b, h_kv, seq_len, head_dim;
    int stride_iz, stride_ih, stride_in;
    int stride_oz, stride_oh, stride_on;

    // allocate
    if (tensor_layout == std::string("HND")) {
        // tensor layout unpack
        b = input_shape[0];
        h_kv = input_shape[1];
        seq_len = input_shape[2];
        head_dim = input_shape[3];

        // stride unpack
        auto tensor_strides = input_tensor.strides();
        stride_iz = tensor_strides[0];
        stride_ih = tensor_strides[1];
        stride_in = tensor_strides[2];

        auto tensor_o_strides = output_tensor.strides();
        stride_oz = tensor_o_strides[0];
        stride_oh = tensor_o_strides[1];
        stride_on = tensor_o_strides[2];

    } else if (tensor_layout == std::string("NHD")) {
        // tensor layout unpack
        b = input_shape[0];
        h_kv = input_shape[2];    // reverse
        seq_len = input_shape[1];
        head_dim = input_shape[3];

        // stride unpack
        auto tensor_strides = input_tensor.strides();
        stride_iz = tensor_strides[0];
        stride_ih = tensor_strides[2];    // reverse
        stride_in = tensor_strides[1];

        auto tensor_o_strides = output_tensor.strides();
        stride_oz = tensor_o_strides[0];
        stride_oh = tensor_o_strides[2];  // reverse
        stride_on = tensor_o_strides[1];
    }
    else {
        throw std::runtime_error("Unsupported tensor layout");
    }

    auto scale_tensor = paddle::empty({b, h_kv, (seq_len + BLK - 1) / BLK * (BLK / WARP) * 4},
                                        paddle::DataType::FLOAT32, x.place());
    int L = seq_len;
    int stride_sz = scale_tensor.strides()[0];
    int stride_sh = scale_tensor.strides()[1];
    int bsz = b;
"""

    op_name = "triton_sageattn_quant_key_per_thread"
    op_name += get_dtype_str(x.dtype)
    op_name += f"_BLK{BLK}_seq{seq_len}_h{h_kv}_dim{head_dim}"

    if op_name not in OpProtoHolder.instance().op_proto_map.keys():
        Output = paddle.empty(x.shape, dtype=paddle.int8)
        Scale = paddle.empty(shape=(b, h_kv, ((seq_len + BLK - 1) // BLK) * (BLK // WARP) * 4), dtype="float32")
        # output_tensor & scale_tensor has beed defined in above areas
        prepare_ptr_for_triton_kernel = """
    // prepare tensor
    CUdeviceptr input_ptrs[3] = {
        get_tensor_ptr(x),
        get_tensor_ptr(output_tensor),
        get_tensor_ptr(scale_tensor)
    };
"""
        return_tensor_names = "output_tensor, scale_tensor"

        template_used = rendering_common_template(
            sageattn_quant_key_per_thread_int8,
            prepare_attr_for_triton_kernel=prepare_attr_for_triton_kernel,
            prepare_ptr_for_triton_kernel=prepare_ptr_for_triton_kernel,
            return_tensor_names=return_tensor_names,
        )
        grid = ("((L + BKG - 1) / BKG) * (BKG / WARP) * 8", "h_kv", "bsz")
        sageattn_quant_key_per_thread_int8_kernel[(op_name, template_used, grid)](
            Input=x,
            Output=Output,
            Scale=Scale,
            L=L,
            stride_iz=stride_bz_k,
            stride_ih=stride_h_k,
            stride_in=stride_seq_k,
            stride_oz=stride_bz_ko,
            stride_oh=stride_h_ko,
            stride_on=stride_seq_ko,
            stride_sz=stride_sz,
            stride_sh=stride_sh,
            bsz=-1,  # grid num, for compiling
            h_kv=h_kv,  # grid num, for compiling
            C=C,
            BLK=WARP,
            WARP=WARP,  # grid num, for compiling
            BKG=BLK,  # grid num, for compiling
        )

    if in_dynamic_or_pir_mode():
        outs = _C_ops._run_custom_op(op_name, x, km, BLK, WARP, sm_scale, tensor_layout)
        return outs[0], outs[1]
    else:
        helper = LayerHelper(op_name, **locals())
        inputs = {"x": x, "km": km}
        out_int8 = helper.create_variable_for_type_inference(dtype=paddle.int8)
        out_scale = helper.create_variable_for_type_inference(dtype=paddle.float32)

        helper.append_op(
            type=op_name,
            inputs=inputs,
            attrs={
                "BLK": BLK,
                "WARP": WARP,
                "sm_scale": sm_scale,
                "tensor_layout": tensor_layout,
            },
            outputs={"output_tensor": out_int8, "scale_tensor": out_scale},
        )
        return out_int8, out_scale


def per_thread_int8(q, k, km=None, BLKQ=128, BLKK=64, WARPQ=32, WARPK=64, sm_scale=None, tensor_layout="HND"):
    q_int8, q_scale = sageattn_quant_query_per_thread_int8(
        q, BLK=BLKQ, WARP=WARPQ, sm_scale=sm_scale, tensor_layout=tensor_layout
    )
    k_int8, k_scale = sageattn_quant_key_per_thread_int8(
        k, km=km, BLK=BLKK, WARP=WARPK, sm_scale=sm_scale, tensor_layout=tensor_layout
    )
    return q_int8, q_scale, k_int8, k_scale


@paddle_use_triton(key=["1"])
def sageattn_attn_fwd_causal_false_kernel(
    Q,
    K,
    V,
    Q_scale,
    K_scale,
    Out,
    Lse,
    stride_qz,
    stride_qh,
    stride_qn,
    stride_kz,
    stride_kh,
    stride_kn,
    stride_vz,
    stride_vh,
    stride_vn,
    stride_oz,
    stride_oh,
    stride_on,
    qo_len,
    kv_len,
    BSZ,
    h_qo: tl.constexpr,
    num_kv_groups: tl.constexpr,
    HEAD_DIM: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    RETURN_LSE: tl.constexpr,
):
    start_m = tl.program_id(0)

    off_z = tl.program_id(2).to(tl.int64)
    off_h = tl.program_id(1).to(tl.int64)

    q_scale_offset = (off_z * h_qo + off_h) * tl.cdiv(qo_len, BLOCK_M)
    k_scale_offset = (off_z * (h_qo // num_kv_groups) + off_h // num_kv_groups) * tl.cdiv(kv_len, BLOCK_N)

    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, HEAD_DIM)
    Q_ptrs = Q + (off_z * stride_qz + off_h * stride_qh) + offs_m[:, None] * stride_qn + offs_k[None, :]
    Q_scale_ptr = Q_scale + q_scale_offset + start_m
    K_ptrs = (
        K + (off_z * stride_kz + (off_h // num_kv_groups) * stride_kh) + offs_n[None, :] * stride_kn + offs_k[:, None]
    )
    K_scale_ptr = K_scale + k_scale_offset
    V_ptrs = (
        V + (off_z * stride_vz + (off_h // num_kv_groups) * stride_vh) + offs_n[:, None] * stride_vn + offs_k[None, :]
    )
    O_block_ptr = Out + (off_z * stride_oz + off_h * stride_oh) + offs_m[:, None] * stride_on + offs_k[None, :]

    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")
    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0
    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)

    q = tl.load(Q_ptrs, mask=offs_m[:, None] < qo_len)
    q_scale = tl.load(Q_scale_ptr)

    # fused zone
    lo, hi = 0, kv_len
    for start_n in range(lo, hi, BLOCK_N):
        start_n = tl.multiple_of(start_n, BLOCK_N)
        k_mask = offs_n[None, :] < (kv_len - start_n)
        k = tl.load(K_ptrs, mask=k_mask)
        k_scale = tl.load(K_scale_ptr)
        qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale
        m_ij = tl.maximum(m_i, tl.max(qk, 1))
        qk = qk - m_ij[:, None]
        p = tl.math.exp2(qk)
        l_ij = tl.sum(p, 1)

        alpha = tl.math.exp2(m_i - m_ij)
        l_i = l_i * alpha + l_ij

        acc = acc * alpha[:, None]

        v = tl.load(V_ptrs, mask=offs_n[:, None] < (kv_len - start_n))
        p = p.to(tl.float16)

        acc += tl.dot(p, v, out_dtype=tl.float16)
        m_i = m_ij
        K_ptrs += BLOCK_N * stride_kn
        K_scale_ptr += 1
        V_ptrs += BLOCK_N * stride_vn
    # zone end

    acc = acc / l_i[:, None]
    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask=(offs_m[:, None] < qo_len))

    if RETURN_LSE:
        lse_ptrs = Lse + (off_z * qo_len * h_qo + off_h * qo_len) + offs_m
        l_i = tl.log2(l_i) + m_i
        tl.store(lse_ptrs, l_i, mask=(offs_m < qo_len))


def sageattn_forward_causal_false(
    q, k, v, q_scale, k_scale, output_dtype="float16", tensor_layout="HND", return_lse=False
):
    """
    [params]
        q: paddle.Tensor, dtype in int8, q tensor after quant.
        k: paddle.Tensor, dtype in int8, k tensor after quant.
        v: paddle.Tensor, dtype in fp16 or bf16, v tensor.
        q_scale: paddle.Tensor, dtype in fp16 or bf16, this is the output tensor for scale factor, from quant kernel.
        k_scale: paddle.Tensor, dtype in fp16 or bf16, this is the output tensor for scale factor, from quant kernel.
        output_dtype: string. Only in ['float16', 'bfloat16']. The datatype of q, k, v tensor.
        tensor_layout: string. Only in ['HND', 'NHD'], 'HND' -> [bsz, num_heads, seq_len, head_dim],
                                                       'NHD' -> [bsz, seq_len, num_heads, head_dim]
        return_lse: bool. Return lse correction or not. Useful in parallel computing. Default False.
    [Examples]
        batch_size = 2
        num_heads = 24
        seq_len = 1376
        head_dim = 64

        sm_scale = 1.0 / (head_dim_og ** 0.5)

        # note: this layout is 'NHD'
        tensor_layout = 'NHD'
        q = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")
        k = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")
        v = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")

        km = paddle.mean(k, axis=seq_dim, keepdim=True)

        q_int8, q_scale, k_int8, k_scale = per_block_int8(q, k, km=km, sm_scale=sm_scale, tensor_layout=tensor_layout)
        o, lse = sageattn_forward_causal_false(q_int8, k_int8, v, q_scale, k_scale,
                                                output_dtype="float16", tensor_layout=tensor_layout)
    """
    assert output_dtype in ["float16", "bfloat16"]

    Out = paddle.empty(q.shape, dtype=output_dtype)
    if tensor_layout == "HND":
        b, h_qo, qo_len, head_dim = q.shape
        _, h_kv, kv_len, _ = k.shape

        stride_qz, stride_qh, stride_qn = h_qo * qo_len * head_dim, qo_len * head_dim, head_dim
        stride_kz, stride_kh, stride_kn = h_kv * kv_len * head_dim, kv_len * head_dim, head_dim
        stride_vz, stride_vh, stride_vn = h_kv * kv_len * head_dim, kv_len * head_dim, head_dim
        stride_oz, stride_oh, stride_on = h_qo * qo_len * head_dim, qo_len * head_dim, head_dim
    elif tensor_layout == "NHD":
        b, qo_len, h_qo, head_dim = q.shape
        _, kv_len, h_kv, _ = k.shape

        stride_qz, stride_qh, stride_qn = qo_len * h_qo * head_dim, head_dim, h_qo * head_dim
        stride_kz, stride_kh, stride_kn = kv_len * h_kv * head_dim, head_dim, h_kv * head_dim
        stride_vz, stride_vh, stride_vn = kv_len * h_kv * head_dim, head_dim, h_kv * head_dim
        stride_oz, stride_oh, stride_on = qo_len * h_qo * head_dim, head_dim, h_qo * head_dim
    else:
        raise ValueError(f"Unknown tensor layout: {tensor_layout}")

    HEAD_DIM_K = head_dim
    num_kv_groups = h_qo // h_kv
    BSZ = b

    prepare_attr_for_triton_kernel = """
    paddle::DataType output_t;
    if (output_dtype == std::string("float16")) {
        output_t = paddle::DataType::FLOAT16;
    } else {
        output_t = paddle::DataType::BFLOAT16;
    }

    auto out_tensor = paddle::empty(q.shape(), output_t, q.place());
    auto q_strides = q.strides();
    auto k_strides = k.strides();
    auto v_strides = v.strides();
    auto o_strides = out_tensor.strides();

    int b, h_qo, qo_len, head_dim;
    int kv_len, h_kv;

    int stride_qz, stride_qh, stride_qn;
    int stride_kz, stride_kh, stride_kn;
    int stride_vz, stride_vh, stride_vn;
    int stride_oz, stride_oh, stride_on;

    if (tensor_layout == "HND") {
        b = q.shape()[0];
        h_qo = q.shape()[1];
        qo_len = q.shape()[2];
        head_dim = q.shape()[3];

        h_kv = k.shape()[1];
        kv_len = k.shape()[2];

        stride_qz = q_strides[0];
        stride_qh = q_strides[1];
        stride_qn = q_strides[2];

        stride_kz = k_strides[0];
        stride_kh = k_strides[1];
        stride_kn = k_strides[2];

        stride_vz = v_strides[0];
        stride_vh = v_strides[1];
        stride_vn = v_strides[2];

        stride_oz = o_strides[0];
        stride_oh = o_strides[1];
        stride_on = o_strides[2];
    } else if (tensor_layout == "NHD") {
        b = q.shape()[0];
        qo_len = q.shape()[1];   // reverse
        h_qo = q.shape()[2];
        head_dim = q.shape()[3];

        kv_len = k.shape()[1];   // reverse
        h_kv = k.shape()[2];

        stride_qz = q_strides[0];
        stride_qh = q_strides[2];       // reverse
        stride_qn = q_strides[1];

        stride_kz = k_strides[0];
        stride_kh = k_strides[2];       // reverse
        stride_kn = k_strides[1];

        stride_vz = v_strides[0];
        stride_vh = v_strides[2];       // reverse
        stride_vn = v_strides[1];

        stride_oz = o_strides[0];
        stride_oh = o_strides[2];       // reverse
        stride_on = o_strides[1];
    } else {
        throw std::runtime_error("Unsupported tensor layout");
    }

    int BSZ = b;
"""

    op_name = "triton_sageattn_attn_fwd_causal_false"
    op_name += get_dtype_str(Out.dtype)
    op_name += f"_seq{qo_len}_h{h_qo}_dim{HEAD_DIM_K}"

    sageattn_attn_fwd_causal_false_config = []
    if head_dim == 64:
        sageattn_attn_fwd_causal_false_config.append({"num_warps": 4, "num_stages": 3})
    else:
        sageattn_attn_fwd_causal_false_config.append({"num_warps": 8, "num_stages": 4})

    if op_name not in OpProtoHolder.instance().op_proto_map.keys():
        if return_lse:
            Lse = paddle.empty((b, h_qo, qo_len), dtype=paddle.float32)
        else:
            Lse = paddle.empty((0, 0, 0), dtype=paddle.float32)
        prepare_ptr_for_triton_kernel = """
        paddle::Tensor lse_tensor;

        if (return_lse) {
            lse_tensor = paddle::empty({b, h_qo, qo_len}, q.dtype(), q.place());
        } else {
            lse_tensor = paddle::empty({1,1,1}, paddle::DataType::FLOAT32, paddle::CPUPlace());
        }

        CUdeviceptr input_ptrs[7] = {
            get_tensor_ptr(q),
            get_tensor_ptr(k),
            get_tensor_ptr(v),
            get_tensor_ptr(q_scale),
            get_tensor_ptr(k_scale),
            get_tensor_ptr(out_tensor),
            get_tensor_ptr(lse_tensor)
        };
    """
        return_tensor_names = "out_tensor, lse_tensor"
        template_used = rendering_common_template(
            sageattn_forward_causal_false,
            prepare_attr_for_triton_kernel=prepare_attr_for_triton_kernel,
            prepare_ptr_for_triton_kernel=prepare_ptr_for_triton_kernel,
            return_tensor_names=return_tensor_names,
        )
        grid = ("(qo_len + BLOCK_M - 1) / BLOCK_M", "h_qo", "BSZ")
        sageattn_attn_fwd_causal_false_kernel[(op_name, template_used, grid, sageattn_attn_fwd_causal_false_config)](
            Q=q,
            K=k,
            V=v,
            Q_scale=q_scale,
            K_scale=k_scale,
            Out=Out,
            Lse=Lse,
            stride_qz=stride_qz,
            stride_qh=stride_qh,
            stride_qn=stride_qn,
            stride_kz=stride_kz,
            stride_kh=stride_kh,
            stride_kn=stride_kn,
            stride_vz=stride_vz,
            stride_vh=stride_vh,
            stride_vn=stride_vn,
            stride_oz=stride_oz,
            stride_oh=stride_oh,
            stride_on=stride_on,
            qo_len=qo_len,
            kv_len=kv_len,
            BSZ=-1,
            h_qo=h_qo,
            num_kv_groups=num_kv_groups,
            HEAD_DIM=HEAD_DIM_K,
            BLOCK_M=128,
            BLOCK_N=64,
            RETURN_LSE=1 if return_lse else 0,
        )

    if in_dynamic_or_pir_mode():
        outs = _C_ops._run_custom_op(op_name, q, k, v, q_scale, k_scale, output_dtype, tensor_layout, return_lse)

        return outs[0], outs[1]
    else:
        helper = LayerHelper(op_name, **locals())
        inputs = {
            "q": q,
            "k": k,
            "v": v,
            "q_scale": q_scale,
            "k_scale": k_scale,
        }
        out_tensor = helper.create_variable_for_type_inference(dtype=Out.dtype)
        out_lse = helper.create_variable_for_type_inference(dtype=Lse.dtype)

        helper.append_op(
            type=op_name,
            inputs=inputs,
            attrs={"output_type": output_dtype, "tensor_layout": tensor_layout, "return_lse": 1 if return_lse else 0},
            outputs={"out_tensor": out_tensor, "lse_tensor": out_lse},
        )

        return out_tensor, out_lse


@paddle_use_triton(key=["1"])
def sageattn_attn_fwd_causal_true_kernel(
    Q,
    K,
    V,
    Q_scale,
    K_scale,
    Out,
    Lse,
    stride_qz,
    stride_qh,
    stride_qn,
    stride_kz,
    stride_kh,
    stride_kn,
    stride_vz,
    stride_vh,
    stride_vn,
    stride_oz,
    stride_oh,
    stride_on,
    qo_len,
    kv_len,
    BSZ,
    h_qo: tl.constexpr,
    num_kv_groups: tl.constexpr,
    HEAD_DIM: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    RETURN_LSE: tl.constexpr,
):
    start_m = tl.program_id(0)

    off_z = tl.program_id(2).to(tl.int64)
    off_h = tl.program_id(1).to(tl.int64)

    q_scale_offset = (off_z * h_qo + off_h) * tl.cdiv(qo_len, BLOCK_M)
    k_scale_offset = (off_z * (h_qo // num_kv_groups) + off_h // num_kv_groups) * tl.cdiv(kv_len, BLOCK_N)

    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, HEAD_DIM)
    Q_ptrs = Q + (off_z * stride_qz + off_h * stride_qh) + offs_m[:, None] * stride_qn + offs_k[None, :]
    Q_scale_ptr = Q_scale + q_scale_offset + start_m
    K_ptrs = (
        K + (off_z * stride_kz + (off_h // num_kv_groups) * stride_kh) + offs_n[None, :] * stride_kn + offs_k[:, None]
    )
    K_scale_ptr = K_scale + k_scale_offset
    V_ptrs = (
        V + (off_z * stride_vz + (off_h // num_kv_groups) * stride_vh) + offs_n[:, None] * stride_vn + offs_k[None, :]
    )
    O_block_ptr = Out + (off_z * stride_oz + off_h * stride_oh) + offs_m[:, None] * stride_on + offs_k[None, :]

    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")
    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0
    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)

    q = tl.load(Q_ptrs, mask=offs_m[:, None] < qo_len)
    q_scale = tl.load(Q_scale_ptr)

    # restore the K_scale_ptr, K_ptrs, V_ptrs
    original_K_scale_ptr = K_scale_ptr
    original_K_ptrs = K_ptrs
    original_V_ptrs = V_ptrs

    # fused zone
    # STAGE == 1
    lo, hi = 0, start_m * BLOCK_M

    for start_n in range(lo, hi, BLOCK_N):
        start_n = tl.multiple_of(start_n, BLOCK_N)
        k_mask = offs_n[None, :] < (kv_len - start_n)
        k = tl.load(K_ptrs, mask=k_mask)
        k_scale = tl.load(K_scale_ptr)
        qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale

        # stage == 1 branch
        m_ij = tl.maximum(m_i, tl.max(qk, 1))
        qk = qk - m_ij[:, None]
        # stage == 1 branch end

        p = tl.math.exp2(qk)
        l_ij = tl.sum(p, 1)

        alpha = tl.math.exp2(m_i - m_ij)
        l_i = l_i * alpha + l_ij

        acc = acc * alpha[:, None]

        v = tl.load(V_ptrs, mask=offs_n[:, None] < (kv_len - start_n))
        p = p.to(tl.float16)

        acc += tl.dot(p, v, out_dtype=tl.float16)
        m_i = m_ij
        K_ptrs += BLOCK_N * stride_kn
        K_scale_ptr += 1
        V_ptrs += BLOCK_N * stride_vn

    # stage == 2
    # restore
    K_scale_ptr = original_K_scale_ptr
    V_ptrs = original_V_ptrs
    K_ptrs = original_K_ptrs

    # begin stage == 2
    lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M
    lo = tl.multiple_of(lo, BLOCK_M)
    K_scale_ptr += lo // BLOCK_N
    K_ptrs += stride_kn * lo
    V_ptrs += stride_vn * lo

    for start_n in range(lo, hi, BLOCK_N):
        start_n = tl.multiple_of(start_n, BLOCK_N)
        k_mask = offs_n[None, :] < (kv_len - start_n)
        k = tl.load(K_ptrs, mask=k_mask)
        k_scale = tl.load(K_scale_ptr)
        qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale

        # stage == 2 branch
        mask = offs_m[:, None] >= (start_n + offs_n[None, :])
        qk = qk + tl.where(mask, 0, -1.0e6)
        m_ij = tl.maximum(m_i, tl.max(qk, 1))
        qk -= m_ij[:, None]
        # stage == 2 branch end

        p = tl.math.exp2(qk)
        l_ij = tl.sum(p, 1)

        alpha = tl.math.exp2(m_i - m_ij)
        l_i = l_i * alpha + l_ij

        acc = acc * alpha[:, None]

        v = tl.load(V_ptrs, mask=offs_n[:, None] < (kv_len - start_n))
        p = p.to(tl.float16)

        acc += tl.dot(p, v, out_dtype=tl.float16)
        m_i = m_ij
        K_ptrs += BLOCK_N * stride_kn
        K_scale_ptr += 1
        V_ptrs += BLOCK_N * stride_vn

    # fused zone end

    acc = acc / l_i[:, None]
    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask=(offs_m[:, None] < qo_len))

    if RETURN_LSE:
        lse_ptrs = Lse + (off_z * qo_len * h_qo + off_h * qo_len) + offs_m
        l_i = tl.log2(l_i) + m_i
        tl.store(lse_ptrs, l_i, mask=(offs_m < qo_len))


def sageattn_forward_causal_true(
    q, k, v, q_scale, k_scale, output_dtype="float16", tensor_layout="HND", return_lse=False
):
    """
    [params]
        q: paddle.Tensor, dtype in int8, q tensor after quant.
        k: paddle.Tensor, dtype in int8, k tensor after quant.
        v: paddle.Tensor, dtype in fp16 or bf16, v tensor.
        q_scale: paddle.Tensor, dtype in fp16 or bf16, this is the output tensor for scale factor, from quant kernel.
        k_scale: paddle.Tensor, dtype in fp16 or bf16, this is the output tensor for scale factor, from quant kernel.
        output_dtype: string. Only in ['float16', 'bfloat16']. The datatype of q, k, v tensor.
        tensor_layout: string. Only in ['HND', 'NHD'], 'HND' -> [bsz, num_heads, seq_len, head_dim],
                                                       'NHD' -> [bsz, seq_len, num_heads, head_dim]
        return_lse: bool. Return lse correction or not. Useful in parallel computing. Default False.
    [Examples]
        batch_size = 2
        num_heads = 24
        seq_len = 1376
        head_dim = 64

        sm_scale = 1.0 / (head_dim_og ** 0.5)

        # note: this layout is 'NHD'
        tensor_layout = 'NHD'
        q = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")
        k = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")
        v = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")

        km = paddle.mean(k, axis=seq_dim, keepdim=True)

        q_int8, q_scale, k_int8, k_scale = per_block_int8(q, k, km=km, sm_scale=sm_scale, tensor_layout=tensor_layout)
        o, lse = sageattn_forward_causal_true(q_int8, k_int8, v, q_scale, k_scale,
                                                output_dtype="float16", tensor_layout=tensor_layout)
    """
    assert output_dtype in ["float16", "bfloat16"]

    Out = paddle.empty(q.shape, dtype=output_dtype)
    if tensor_layout == "HND":
        b, h_qo, qo_len, head_dim = q.shape
        _, h_kv, kv_len, _ = k.shape

        stride_qz, stride_qh, stride_qn = h_qo * qo_len * head_dim, qo_len * head_dim, head_dim
        stride_kz, stride_kh, stride_kn = h_kv * kv_len * head_dim, kv_len * head_dim, head_dim
        stride_vz, stride_vh, stride_vn = h_kv * kv_len * head_dim, kv_len * head_dim, head_dim
        stride_oz, stride_oh, stride_on = h_qo * qo_len * head_dim, qo_len * head_dim, head_dim
    elif tensor_layout == "NHD":
        b, qo_len, h_qo, head_dim = q.shape
        _, kv_len, h_kv, _ = k.shape

        stride_qz, stride_qh, stride_qn = qo_len * h_qo * head_dim, head_dim, h_qo * head_dim
        stride_kz, stride_kh, stride_kn = kv_len * h_kv * head_dim, head_dim, h_kv * head_dim
        stride_vz, stride_vh, stride_vn = kv_len * h_kv * head_dim, head_dim, h_kv * head_dim
        stride_oz, stride_oh, stride_on = qo_len * h_qo * head_dim, head_dim, h_qo * head_dim
    else:
        raise ValueError(f"Unknown tensor layout: {tensor_layout}")

    HEAD_DIM_K = head_dim
    num_kv_groups = h_qo // h_kv
    BSZ = b

    prepare_attr_for_triton_kernel = """
    paddle::DataType output_t;
    if (output_dtype == std::string("float16")) {
        output_t = paddle::DataType::FLOAT16;
    } else {
        output_t = paddle::DataType::BFLOAT16;
    }
    auto out_tensor = paddle::empty(q.shape(), output_t, q.place());
    auto q_strides = q.strides();
    auto k_strides = k.strides();
    auto v_strides = v.strides();
    auto o_strides = out_tensor.strides();

    int b, h_qo, qo_len, head_dim;
    int kv_len, h_kv;

    int stride_qz, stride_qh, stride_qn;
    int stride_kz, stride_kh, stride_kn;
    int stride_vz, stride_vh, stride_vn;
    int stride_oz, stride_oh, stride_on;

    if (tensor_layout == "HND") {
        b = q.shape()[0];
        h_qo = q.shape()[1];
        qo_len = q.shape()[2];
        head_dim = q.shape()[3];

        h_kv = k.shape()[1];
        kv_len = k.shape()[2];

        stride_qz = q_strides[0];
        stride_qh = q_strides[1];
        stride_qn = q_strides[2];

        stride_kz = k_strides[0];
        stride_kh = k_strides[1];
        stride_kn = k_strides[2];

        stride_vz = v_strides[0];
        stride_vh = v_strides[1];
        stride_vn = v_strides[2];

        stride_oz = o_strides[0];
        stride_oh = o_strides[1];
        stride_on = o_strides[2];
    } else if (tensor_layout == "NHD") {
        b = q.shape()[0];
        qo_len = q.shape()[1];   // reverse
        h_qo = q.shape()[2];
        head_dim = q.shape()[3];

        kv_len = k.shape()[1];   // reverse
        h_kv = k.shape()[2];

        stride_qz = q_strides[0];
        stride_qh = q_strides[2];       // reverse
        stride_qn = q_strides[1];

        stride_kz = k_strides[0];
        stride_kh = k_strides[2];       // reverse
        stride_kn = k_strides[1];

        stride_vz = v_strides[0];
        stride_vh = v_strides[2];       // reverse
        stride_vn = v_strides[1];

        stride_oz = o_strides[0];
        stride_oh = o_strides[2];       // reverse
        stride_on = o_strides[1];
    } else {
        throw std::runtime_error("Unsupported tensor layout");
    }

    int HEAD_DIM_K = head_dim;
    int num_kv_groups = h_qo / h_kv;
    int BSZ = b;
"""

    op_name = "triton_sageattn_attn_fwd_causal_true"
    op_name += get_dtype_str(Out.dtype)
    op_name += f"_seq{qo_len}_h{h_qo}_dim{HEAD_DIM_K}"

    sageattn_attn_fwd_causal_true_config = []
    if head_dim == 64:
        sageattn_attn_fwd_causal_true_config.append({"num_warps": 4, "num_stages": 4})
    else:
        sageattn_attn_fwd_causal_true_config.append({"num_warps": 8, "num_stages": 4})

    if op_name not in OpProtoHolder.instance().op_proto_map.keys():
        if return_lse:
            Lse = paddle.empty((b, h_qo, qo_len), dtype=paddle.float32)
        else:
            Lse = paddle.empty((0, 0, 0), dtype=paddle.float32)
        prepare_ptr_for_triton_kernel = """
        paddle::Tensor lse_tensor;

        if (return_lse) {
            lse_tensor = paddle::empty({b, h_qo, qo_len}, q.dtype(), q.place());
        } else {
            lse_tensor = paddle::empty({1,1,1}, paddle::DataType::FLOAT32, paddle::CPUPlace());
        }

        CUdeviceptr input_ptrs[7] = {
            get_tensor_ptr(q),
            get_tensor_ptr(k),
            get_tensor_ptr(v),
            get_tensor_ptr(q_scale),
            get_tensor_ptr(k_scale),
            get_tensor_ptr(out_tensor),
            get_tensor_ptr(lse_tensor)
        };
    """
        return_tensor_names = "out_tensor, lse_tensor"
        template_used = rendering_common_template(
            sageattn_forward_causal_true,
            prepare_attr_for_triton_kernel=prepare_attr_for_triton_kernel,
            prepare_ptr_for_triton_kernel=prepare_ptr_for_triton_kernel,
            return_tensor_names=return_tensor_names,
        )
        grid = ("(qo_len + BLOCK_M - 1) / BLOCK_M", "h_qo", "BSZ")
        sageattn_attn_fwd_causal_true_kernel[(op_name, template_used, grid, sageattn_attn_fwd_causal_true_config)](
            Q=q,
            K=k,
            V=v,
            Q_scale=q_scale,
            K_scale=k_scale,
            Out=Out,
            Lse=Lse,
            stride_qz=stride_qz,
            stride_qh=stride_qh,
            stride_qn=stride_qn,
            stride_kz=stride_kz,
            stride_kh=stride_kh,
            stride_kn=stride_kn,
            stride_vz=stride_vz,
            stride_vh=stride_vh,
            stride_vn=stride_vn,
            stride_oz=stride_oz,
            stride_oh=stride_oh,
            stride_on=stride_on,
            qo_len=qo_len,
            kv_len=kv_len,
            BSZ=-1,
            h_qo=h_qo,
            num_kv_groups=num_kv_groups,
            HEAD_DIM=HEAD_DIM_K,
            BLOCK_M=128,
            BLOCK_N=64,
            RETURN_LSE=1 if return_lse else 0,
        )

    if in_dynamic_or_pir_mode():
        outs = _C_ops._run_custom_op(op_name, q, k, v, q_scale, k_scale, output_dtype, tensor_layout, return_lse)

        return outs[0], outs[1]
    else:
        helper = LayerHelper(op_name, **locals())
        inputs = {
            "q": q,
            "k": k,
            "v": v,
            "q_scale": q_scale,
            "k_scale": k_scale,
        }
        out_tensor = helper.create_variable_for_type_inference(dtype=Out.dtype)
        out_lse = helper.create_variable_for_type_inference(dtype=Lse.dtype)

        helper.append_op(
            type=op_name,
            inputs=inputs,
            attrs={"output_type": output_dtype, "tensor_layout": tensor_layout, "return_lse": 1 if return_lse else 0},
            outputs={"out_tensor": out_tensor, "lse_tensor": out_lse},
        )

        return out_tensor, out_lse


# ============== sage attention triton API =================
def sageattn_qk_int8_pv_fp16_triton(
    q: paddle.Tensor,
    k: paddle.Tensor,
    v: paddle.Tensor,
    tensor_layout: str = "HND",
    is_causal: bool = False,
    sm_scale: Optional[float] = None,
    smooth_k: bool = True,
    return_lse: bool = False,
) -> paddle.Tensor:
    """
    Examples:
        batch_size = 2
        num_heads = 24
        seq_len = 1376
        head_dim = 64

        q = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")
        k = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")
        v = paddle.randn(shape=(batch_size, seq_len, num_heads, head_dim), dtype="float16")
        sm_scale = 1 / (head_dim ** 0.5)

        o = paddlemix.triton_ops.sageattn_qk_int8_pv_fp16_triton(q, k, v, tensor_layout="NHD", is_causal=False, sm_scale=sm_scale, smooth_k=True, return_lse=False)
    """
    dtype = q.dtype
    assert dtype in [
        paddle.float16,
        paddle.bfloat16,
    ], "Input tensors must be in dtype of torch.float16 or torch.bfloat16"
    assert (
        str(q.place) == str(k.place) == str(v.place)
    ), f"All tensors must be on the same device. Got q: {q.place}, k: {k.place}, v: {v.place}"
    assert q.dtype == k.dtype == v.dtype, "All tensors must have the same dtype."

    head_dim_og = q.shape[-1]
    # if not 64 or 128, then fill to 64 or 128
    if head_dim_og < 64:
        q = paddle.nn.functional.pad(q, pad=[0, 64 - head_dim_og])
        k = paddle.nn.functional.pad(k, pad=[0, 64 - head_dim_og])
        v = paddle.nn.functional.pad(v, pad=[0, 64 - head_dim_og])
    elif head_dim_og > 64 and head_dim_og < 128:
        q = paddle.nn.functional.pad(q, pad=[0, 128 - head_dim_og])
        k = paddle.nn.functional.pad(k, pad=[0, 128 - head_dim_og])
        v = paddle.nn.functional.pad(v, pad=[0, 128 - head_dim_og])
    elif head_dim_og > 128:
        raise ValueError(f"Unsupported head_dim: {head_dim_og}")

    seq_dim = 1 if tensor_layout == "NHD" else 2

    if smooth_k:
        km = paddle.mean(k, axis=seq_dim, keepdim=True)
        if return_lse:
            if tensor_layout == "NHD":
                lse_correction = paddle.matmul(
                    paddle.transpose(q, [0, 2, 1, 3]), paddle.squeeze(paddle.transpose(q, [0, 2, 3, 1]), axis=-1)
                ).astype(paddle.float32)
            else:
                lse_correction = paddle.matmul(q, paddle.squeeze(paddle.transpose(km, [0, 1, 3, 2]), axis=-1)).astype(
                    paddle.float32
                )
    else:
        km = None

    if dtype == paddle.bfloat16 or dtype == paddle.float32:
        v = paddle.cast(v, dtype=paddle.float16)

    if sm_scale is None:
        sm_scale = 1.0 / (head_dim_og**0.5)

    q_int8, q_scale, k_int8, k_scale = per_block_int8(q, k, km=km, sm_scale=sm_scale, tensor_layout=tensor_layout)

    output_dtype = "float16" if dtype == paddle.float16 else "bfloat16"

    if is_causal:
        o, lse = sageattn_forward_causal_true(
            q_int8,
            k_int8,
            v,
            q_scale,
            k_scale,
            output_dtype=output_dtype,
            tensor_layout=tensor_layout,
            return_lse=return_lse,
        )
    else:
        o, lse = sageattn_forward_causal_false(
            q_int8,
            k_int8,
            v,
            q_scale,
            k_scale,
            output_dtype=output_dtype,
            tensor_layout=tensor_layout,
            return_lse=return_lse,
        )

    o = o[..., :head_dim_og]

    if return_lse:
        return o, lse / 1.44269504 + lse_correction * sm_scale if smooth_k else lse / 1.44269504
    else:
        return o
