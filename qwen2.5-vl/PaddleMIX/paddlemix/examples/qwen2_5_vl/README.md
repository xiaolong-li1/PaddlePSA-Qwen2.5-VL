# Qwen2.5-VL

## 1. 模型介绍

[Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL) 是 Qwen 团队推出的一个专注于视觉与语言（Vision-Language, VL）任务的多模态大模型。它旨在通过结合图像和文本信息，提供强大的跨模态理解能力，可以处理涉及图像描述、视觉问答（VQA）、图文检索等多种任务。

**主要增强功能：**

**强大的文档解析能力：** 将文本识别升级为全文档解析，能够出色地处理多场景、多语言、各类内置（手写、表格、图表、化学式、乐谱）文档。

**跨格式的精确物体定位：** 提高检测、指向和计数物体的准确度，适应绝对坐标和 JSON 格式，实现高级空间推理。

**超长视频理解和细粒度视频解析：** 将原生动态分辨率扩展到时间维度，增强理解数小时视频的能力，同时在数秒内提取事件片段。

**增强计算机和移动设备的代理功能：** 利用先进的基础、推理和决策能力，通过智能手机和计算机上的卓越代理功能增强模型。

**本仓库支持的模型权重:**

| Model              |
|--------------------|
| Qwen/Qwen2.5-VL-3B-Instruct  |
| Qwen/Qwen2.5-VL-7B-Instruct  |
| Qwen/Qwen2.5-VL-72B-Instruct  |

注意：与huggingface权重同名，但权重为paddle框架的Tensor，使用`xxx.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct")`即可自动下载该权重文件夹到缓存目录。


## 2 环境准备
1）[安装PaddlePaddle](https://github.com/PaddlePaddle/PaddleMIX?tab=readme-ov-file#3-%EF%B8%8F%E5%AE%89%E8%A3%85paddlepaddle)
- **python >= 3.10**
- **paddlepaddle-gpu 要求是3.0.0b2或develop版本**
```bash
# 提供三种 PaddlePaddle 安装命令示例，也可参考PaddleMIX主页的安装教程进行安装

# 3.0.0b2版本安装示例 (CUDA 11.8)
python -m pip install paddlepaddle-gpu==3.0.0b2 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/

# Develop 版本安装示例
python -m pip install paddlepaddle-gpu==0.0.0.post118 -f https://www.paddlepaddle.org.cn/whl/linux/gpu/develop.html

# sh 脚本快速安装
sh build_paddle_env.sh
```

2）[安装PaddleMIX环境依赖包](https://github.com/PaddlePaddle/PaddleMIX?tab=readme-ov-file#3-%EF%B8%8F%E5%AE%89%E8%A3%85paddlepaddle)
- **paddlenlp >= 3.0.0b3**

```bash
# 提供两种 PaddleMIX 依赖安装命令示例

# pip 安装示例，安装paddlemix、ppdiffusers、项目依赖、paddlenlp
python -m pip install -e . --user
python -m pip install -e ppdiffusers --user
python -m pip install -r requirements.txt --user
python -m pip install paddlenlp==3.0.0b3 --user

# sh 脚本快速安装
sh build_env.sh
```

> 注：
* 请确保安装了以上依赖，否则无法运行。同时，需要安装 paddlemix/external_ops 下的自定义OP, `python setup.py install`。如果安装后仍然找不到算子，需要额外设置PYTHONPATH
* (默认开启flash_attn)使用flash_attn 要求A100/A800显卡或者H20显卡。V100请用float16推理。

## 3 推理预测

### a. 单图预测 (单卡 32G A卡V卡 显存可运行3B模型)
```bash
CUDA_VISIBLE_DEVICES=0 python paddlemix/examples/qwen2_5_vl/single_image_infer.py
```

### b. 多图预测 (单卡 32G A卡V卡 显存可运行3B模型)
```bash
CUDA_VISIBLE_DEVICES=0 python paddlemix/examples/qwen2_5_vl/multi_image_infer.py
```

### c. 视频预测 (单卡 32G A卡V卡 显存可运行3B模型)
```bash
CUDA_VISIBLE_DEVICES=0 python paddlemix/examples/qwen2_5_vl/video_infer.py
```

### d. batch推理 (单卡 32G A卡V卡 显存可运行3B模型)
```bash
CUDA_VISIBLE_DEVICES=0 python paddlemix/examples/qwen2_5_vl/single_image_batch_infer.py
```
### 模型推理支持分布式推理

```bash
# 3B (多卡 32G A卡V卡 显存可运行3B模型)
sh paddlemix/examples/qwen2_5_vl/shell/distributed_qwen2_5_vl_infer_3B.sh
# 7B (多卡 40G A卡 显存可运行7B模型)
sh paddlemix/examples/qwen2_5_vl/shell/distributed_qwen2_5_vl_infer_7B.sh
# 72B (多卡 40G A卡 显存可运行72B模型)
sh paddlemix/examples/qwen2_5_vl/shell/distributed_qwen2_5_vl_infer_72B.sh
```
> ⚠️注意："mp_degree"需要根据显卡数量"gpus"进行调整，例如2卡推理，则设置为2。

## 4 模型微调

### 4.1 小型示例数据集

PaddleMIX团队整理了`chartqa`和`LaTeX_OCR`数据集作为小型的示例数据集，下载链接为：

```bash
wget https://paddlenlp.bj.bcebos.com/models/community/paddlemix/benchmark/playground.tar # 1.0G
wget https://paddlenlp.bj.bcebos.com/datasets/paddlemix/playground/LaTeX_OCR.tar # 1.7G
```
playground/目录下包括了图片目录`data/chartqa/`和标注目录`opensource_json/`，详见`paddlemix/examples/qwen2_5_vl/configs/demo_chartqa_500.json`。
LaTeX_OCR/目录下包括了图片目录和标注文件，详见`paddlemix/examples/qwen2_5_vl/configs/LaTeX_OCR.json`。
训练时只需修改对应shell脚本中的`meta_path`参数即可。如`meta_path="paddlemix/examples/qwen2_5_vl/configs/demo_chartqa_500.json"`。


### 4.2 大型公开数据集

大型的数据集选择6个公开的数据集组合，包括`dvqa`、`chartqa`、`ai2d`、`docvqa`、`geoqa+`、`synthdog_en`，详见`paddlemix/examples/qwen2_5_vl/configs/baseline_6data_330k.json`

PaddleMIX团队整理后的下载链接为：
```bash
wget https://paddlenlp.bj.bcebos.com/datasets/paddlemix/playground.tar # 50G
wget https://paddlenlp.bj.bcebos.com/datasets/paddlemix/playground/opensource_json.tar
```

注意：若先下载了示例数据集的`playground.tar`解压了，此处需删除后，再下载公开数据集的`playground.tar`并解压，opensource_json.tar需下载解压在playground/目录下，opensource_json 里是数据标注的json格式文件。

### 4.3 微调命令

注意：
1）此微调训练为语言模型微调，冻结视觉编码器而放开LLM训练。
2）默认总bs=32，每卡bs=2，gradient_accumulation_steps=2，默认分布式训练配置为"paddle sharding stage2"策略，对应于“torch DeepSpeed ZeRO-2"策略。在LaTeX_OCR数据集训练下，2B模型微调训练的显存大小约为30G，7B模型全量微调训练的显存大小约为55G。***若训练数据集平均分辨率较大时，显存会进一步增加。***
3）若默认训练配置下显存不足，可以调节训练shell脚本中的参数，如```PER_DEVICE_BATCH_SIZE=${PER_DEVICE_BATCH_SIZE:-1}```改小每卡bs为1，以及选择"paddle sharding stage3"策略```--sharding="stage3"```。


```bash
# 3B (单张40G A卡 显存可运行3B模型)
sh paddlemix/examples/qwen2_5_vl/shell/baseline_3b_lora_bs32_1e8_1mp.sh

# 3B (多张40G A卡 显存可运行3B模型)
sh paddlemix/examples/qwen2_5_vl/shell/baseline_3b_bs32_1e8.sh

# 3B lora (多张40G A卡 显存可运行3B模型)
sh paddlemix/examples/qwen2_5_vl/shell/baseline_3b_lora_bs32_1e8.sh

# 7B (多张80G A卡 显存可运行7B模型)
sh paddlemix/examples/qwen2_5_vl/shell/baseline_7b_bs32_1e8.sh

# 7B lora (多张80G A卡 显存可运行7B模型)
sh paddlemix/examples/qwen2_5_vl/shell/baseline_7b_lora_bs32_1e8.sh
```

### 4.4 微调后使用

同按步骤3中的模型推理预测，只需将`paddlemix/examples/qwen2_5_vl/single_image_infer.py`中的`--model_path`参数修改为微调后的模型路径即可。

```bash
CUDA_VISIBLE_DEVICES=0 python paddlemix/examples/qwen2_5_vl/single_image_infer.py
```

## 5 模型权重转换

### 5.1 Torch 转 Paddle
* 安装PyTorch
```
# 安装PyTorch CPU版本
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cpu
```
* 转换权重
```bash
# model_size 3B或7B
# src_dir 为PyTorch 权重，从hugingface中下载 https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct
# dst_dir 为Paddle 权重保存路径
python paddlemix/examples/qwen2_5_vl/torch2paddle_weight_tran.py \
  --src_dir /path/to/source_model \
  --dest_dir /path/to/save_model \
  --model_size 3B \
  --overwrite \
  --skip_config_update \
```

* 替换tokenizer_config.json 中的chat_template为以下模版
```
  "chat_template": "{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message['role'] }}\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}",
```

## 参考文献
```BibTeX
@article{Qwen2.5-VL,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}
```
